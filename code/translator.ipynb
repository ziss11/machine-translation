{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Translation Model**"
      ],
      "metadata": {
        "id": "FXhimN1gKZzg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56aTseK4q9ol",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load Imports\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pathlib\n",
        "\n",
        "from google.colab import drive, files #if use colab\n",
        "from tensorflow.nn import relu, tanh, softmax\n",
        "from tensorflow.lite.python import interpreter\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tz4aY7oDbyeg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Connect with Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZtMplOQv6Lz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Git Clone\n",
        "git_dir = '/content/IOH-Chat-App'\n",
        "git_url = 'https://github.com/bangkit-team/IOH-chat-app.git'\n",
        "\n",
        "if not os.path.exists(git_dir):\n",
        "  subprocess.call(['git', 'clone', git_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN2M4PirLEQB"
      },
      "outputs": [],
      "source": [
        "filedir1 = '/content/IOH-chat-app/MachineLearning/datasets/translation/result/eng-ind.csv'\n",
        "filedir2 = '/content/IOH-chat-app/MachineLearning/datasets/spam/emails.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTSHYGqdTyPk"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv(filedir1)\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(filedir2)\n",
        "df2 = df2.drop(columns='spam')\n",
        "df2 = df2.rename(columns={'text': 'English', 'teks': 'Indonesia'})\n",
        "df2"
      ],
      "metadata": {
        "id": "3u1k0G1oKBSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2])\n",
        "df"
      ],
      "metadata": {
        "id": "Z-shI2BHLTEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgUUgmronChh"
      },
      "outputs": [],
      "source": [
        "start_mark = '<start>'\n",
        "end_mark = '<end>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA5VqxGKtrWx"
      },
      "outputs": [],
      "source": [
        "class TranslatorDataset():\n",
        "  def __init__(self, dataframe):\n",
        "    self.dataframe = dataframe\n",
        "    self.input_tokenizer = None\n",
        "    self.target_tokenizer = None\n",
        "    self._load_data_from_file()\n",
        "\n",
        "  def _load_data_from_file(self):\n",
        "    df = self.dataframe\n",
        "\n",
        "    input_lang = df.English.values\n",
        "    target_lang = df.Indonesia.values\n",
        "\n",
        "    return input_lang, target_lang\n",
        "\n",
        "  def _normalize_and_preprocess(self, text, use_mark=False):\n",
        "    if use_mark:\n",
        "      text = text.strip()\n",
        "      text = ' '.join([start_mark, text, end_mark])\n",
        "    else:\n",
        "      text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "  def _tokenize(self, sentences, num_words, maxlen):\n",
        "    punctuation = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=num_words, filters=punctuation)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    sequences = pad_sequences(\n",
        "        sequences, maxlen=self.maxlen, padding='post', truncating='post')\n",
        "\n",
        "    return sequences, tokenizer\n",
        "\n",
        "  def _create_dataset(self):\n",
        "    input_lang, target_lang = self._load_data_from_file()\n",
        "\n",
        "    input_sentence = np.array(\n",
        "        list(map(lambda x: self._normalize_and_preprocess(x, False), input_lang)))\n",
        "    \n",
        "    target_sentence = np.array(\n",
        "        list(map(lambda y: self._normalize_and_preprocess(y, True), target_lang)))\n",
        "    \n",
        "    return input_sentence, target_sentence\n",
        "\n",
        "  def _load_dataset(self, num_words):\n",
        "    input_lang, target_lang = self._create_dataset()\n",
        "\n",
        "    self.maxlen = 20\n",
        "    self.buffer_size = len(input_lang)\n",
        "\n",
        "    input_sequences, input_tokenizer = self._tokenize(\n",
        "        input_lang, num_words, self.maxlen)\n",
        "    \n",
        "    target_sequences, target_tokenizer = self._tokenize(\n",
        "        target_lang, num_words, self.maxlen,)\n",
        "\n",
        "    return (input_sequences, input_tokenizer), (target_sequences, target_tokenizer)\n",
        "  \n",
        "  def get(self, num_words, batch_size):\n",
        "    input, target = self._load_dataset(num_words)\n",
        "\n",
        "    input_sequences, self.input_tokenizer = input\n",
        "    target_sequences, self.target_tokenizer = target\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
        "    dataset = dataset.shuffle(self.buffer_size).batch(batch_size, drop_remainder=True)\n",
        "    dataset = dataset.cache().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return self.input_tokenizer, self.target_tokenizer, dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lYoZZjqXF__"
      },
      "outputs": [],
      "source": [
        "num_words = 50000\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW7fD1GdrExy"
      },
      "outputs": [],
      "source": [
        "translator_dataset = TranslatorDataset(df)\n",
        "\n",
        "input_tokenizer, target_tokenizer, dataset = translator_dataset.get(num_words, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lQitbaJXt4O"
      },
      "outputs": [],
      "source": [
        "input_batch, target_batch = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j91LroJX50x"
      },
      "outputs": [],
      "source": [
        "input_batch.shape, target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI-TplUE22Tk"
      },
      "outputs": [],
      "source": [
        "input_vocab_size = len(input_tokenizer.index_word) + 1\n",
        "target_vocab_size = len(target_tokenizer.index_word) + 1\n",
        "input_maxlen = input_batch.shape[1]\n",
        "target_maxlen = target_batch.shape[1]\n",
        "\n",
        "input_maxlen, target_maxlen, input_vocab_size, target_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPp4UAxdxemM"
      },
      "outputs": [],
      "source": [
        "input_example = input_batch[-1]\n",
        "input_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r5qpp8Hxjzk"
      },
      "outputs": [],
      "source": [
        "target_example = target_batch[-1]\n",
        "target_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulo132GvOX0l"
      },
      "outputs": [],
      "source": [
        "input_sentence = input_tokenizer.sequences_to_texts([input_example.numpy()])[0]\n",
        "input_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X59pOM0qOtyb"
      },
      "outputs": [],
      "source": [
        "target_sentence = target_tokenizer.sequences_to_texts([target_example.numpy()])[0]\n",
        "target_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeQhJSGasf4P"
      },
      "outputs": [],
      "source": [
        "embed_dims = 1000\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc2nYdHM0Xv0"
      },
      "outputs": [],
      "source": [
        "class Encoder():\n",
        "  def __init__(self, input_vocab_size, embedding_dims, units):\n",
        "    self.units = units\n",
        "    self.batch_size = batch_size\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "\n",
        "    self.embedding = layers.Embedding(self.input_vocab_size, self.embedding_dims)\n",
        "    self.lstm_layer = layers.LSTM(self.units,\n",
        "                                 return_sequences=True,\n",
        "                                 return_state=True,\n",
        "                                 recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedding = self.embedding(inputs)\n",
        "    encoder = self.lstm_layer(embedding)\n",
        "\n",
        "    return encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFONKQDzUdmw"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.w1 = layers.Dense(units, use_bias=True) \n",
        "    self.w2 = layers.Dense(units, use_bias=True) \n",
        "    self.fd = layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    score = self.fd(tanh(\n",
        "        self.w1(query_with_time_axis) + self.w2(values)))\n",
        "\n",
        "    attention_weights = softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ayl_3A_1-IT"
      },
      "outputs": [],
      "source": [
        "class Decoder():\n",
        "  def __init__(self, output_vocab_size, embedding_dims, units):\n",
        "    self.units = units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dims = embedding_dims\n",
        "\n",
        "    self.embedding = layers.Embedding(self.output_vocab_size, self.embedding_dims)\n",
        "    self.lstm_layer = layers.LSTM(self.units,\n",
        "                                  return_sequences=True,\n",
        "                                  return_state=True,\n",
        "                                  recurrent_initializer='glorot_uniform')\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "    self.concat = layers.Concatenate()\n",
        "    self.dense1 = layers.Dense(self.units, activation=tanh, use_bias=False)\n",
        "    self.dropout = layers.Dropout(.5)\n",
        "    self.dense2 = layers.TimeDistributed(layers.Dense(self.output_vocab_size))\n",
        "\n",
        "  def call(self, inputs, en_outputs, state):\n",
        "    embedding = self.embedding(inputs)\n",
        "    dec_outputs, dec_h_state, dec_c_state = self.lstm_layer(\n",
        "        embedding, initial_state=state)\n",
        "    \n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=dec_outputs, values=en_outputs)\n",
        "    \n",
        "    context_and_rnn_output = self.concat(\n",
        "        [context_vector, dec_outputs])\n",
        "\n",
        "    attention_vector = self.dense1(context_and_rnn_output)\n",
        "    outputs = self.dropout(attention_vector)\n",
        "    outputs = self.dense2(outputs)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kio22M03Yp9S"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_vocab_size, embed_dims, units)\n",
        "en_outputs, en_h_state, en_c_state = encoder.call(input_batch)\n",
        "\n",
        "en_outputs.shape, en_h_state.shape, en_c_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_7A5-GTadcM"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(target_vocab_size, embed_dims, units)\n",
        "dec_outputs = decoder.call(target_batch, en_outputs, [en_h_state, en_c_state])\n",
        "\n",
        "dec_outputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdJEFRmxg0zW"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "epochs = 30\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "       from_logits=True, reduction='none')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ifmblhR4X8i"
      },
      "outputs": [],
      "source": [
        "class TranslatorModel():\n",
        "  def __init__(self, input_vocab_size, \n",
        "               target_vocab_size, \n",
        "               embed_dims, \n",
        "               units, \n",
        "               maxlen):\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.embed_dims = embed_dims\n",
        "    self.units = units\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "    self.encoder = Encoder(\n",
        "        self.input_vocab_size, self.embed_dims, self.units)\n",
        "    \n",
        "    self.decoder = Decoder(\n",
        "        self.target_vocab_size, self.embed_dims, self.units)\n",
        "  \n",
        "  def build_model(self):\n",
        "    en_inputs = layers.Input(shape=(self.maxlen,))\n",
        "\n",
        "    en_output, en_h_state, en_c_state = self.encoder.call(en_inputs)\n",
        "\n",
        "    dec_outputs = self.decoder.call(\n",
        "        en_inputs, en_output, [en_h_state, en_c_state])\n",
        "\n",
        "    model = Model(inputs=[en_inputs], \n",
        "                  outputs=[dec_outputs])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsTiGR8daD4K"
      },
      "outputs": [],
      "source": [
        "translator_model = TranslatorModel(\n",
        "    input_vocab_size,\n",
        "    target_vocab_size,\n",
        "    embed_dims,\n",
        "    units,\n",
        "    input_maxlen\n",
        ")\n",
        "model = translator_model.build_model()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOUD1xKgIDH3"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'checkpoint/cp.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='loss',\n",
        "    patience=3, \n",
        "    verbose=1)\n",
        "\n",
        "callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    monitor='loss', \n",
        "    verbose=1, \n",
        "    save_weights_only=True, \n",
        "    save_best_only=True)\n",
        "\n",
        "callbacks = [callback_early_stopping,\n",
        "             callback_checkpoint]\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daAENyOqwdjZ"
      },
      "outputs": [],
      "source": [
        "model.fit(dataset,\n",
        "          epochs=epochs,\n",
        "          callbacks=callbacks,\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QGxWYuiQPnv"
      },
      "outputs": [],
      "source": [
        "saved_model_path  = '/content/drive/MyDrive/Company Case Bangkit/TranslationModel/saved_model'\n",
        "saved_model_dir = os.path.dirname(saved_model_path)\n",
        "\n",
        "if not os.path.exists(saved_model_dir):\n",
        "  shutil.rmtree(saved_model_dir)\n",
        "  \n",
        "model.save(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokenizer_dir = '/content/drive/MyDrive/Company Case Bangkit/TranslationModel/input_tokenizer.json'\n",
        "\n",
        "input_tokenizer_json = input_tokenizer.to_json()\n",
        "with open(input_tokenizer_dir, 'w', encoding='utf-8') as f:\n",
        "    json.dump(input_tokenizer_json, f, ensure_ascii=False)\n",
        "\n",
        "files.download(input_tokenizer_dir)"
      ],
      "metadata": {
        "id": "am-Q49eTM-3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_tokenizer_dir = '/content/drive/MyDrive/Company Case Bangkit/TranslationModel/target_tokenizer.json'\n",
        "\n",
        "target_tokenizer_json = target_tokenizer.to_json()\n",
        "with open(target_tokenizer_dir, 'w', encoding='utf-8') as f:\n",
        "    json.dump(target_tokenizer_json, f, ensure_ascii=False)\n",
        "\n",
        "files.download(target_tokenizer_dir)"
      ],
      "metadata": {
        "id": "0f7HIf8RNCvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YmcvYUyzqLd"
      },
      "outputs": [],
      "source": [
        "class Translator():\n",
        "  def __init__(self, model_path, input_tokenizer_json, target_tokenizer_json, maxlen):\n",
        "    self.model_path = model_path\n",
        "    self.input_tokenizer_json = input_tokenizer_json\n",
        "    self.target_tokenizer_json = target_tokenizer_json\n",
        "    self.maxlen = maxlen\n",
        "\n",
        "    self._load_model()\n",
        "    self._load_tokenizer()\n",
        "\n",
        "  def _load_model(self):\n",
        "    self.model = tf.keras.models.load_model(self.model_path, compile=True)\n",
        "  \n",
        "  def _load_tokenizer(self):\n",
        "    with open(self.input_tokenizer_json) as f:\n",
        "      input_json = json.load(f)\n",
        "      self.input_tokenizer = tokenizer_from_json(input_json)\n",
        "\n",
        "    with open(self.target_tokenizer_json) as f:\n",
        "      target_json = json.load(f)\n",
        "      self.target_tokenizer = tokenizer_from_json(target_json)\n",
        "\n",
        "  def _normalize_and_preprocess(self, text):\n",
        "    punctuation = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "    \n",
        "    text = text.lower().strip()\n",
        "    text = ''.join((filter(lambda x: x not in punctuation, text)))\n",
        "\n",
        "    return text\n",
        "\n",
        "  def __call__(self, sentence):\n",
        "    index_prediction = list()\n",
        "\n",
        "    normalize_sentence = self._normalize_and_preprocess(sentence)\n",
        "    sequences = self.input_tokenizer.texts_to_sequences([normalize_sentence])\n",
        "    sequences = pad_sequences(\n",
        "        sequences, maxlen=self.maxlen, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    predictions = self.model(sequences)\n",
        "\n",
        "    for i in predictions[0]:\n",
        "        index_prediction.append(np.argmax(i))\n",
        "\n",
        "    marks = [start_mark, end_mark]\n",
        "    result = self.target_tokenizer.sequences_to_texts([index_prediction])[0]\n",
        "\n",
        "    result = ' '.join([word for word in result.split(' ') if word not in marks])\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkNd7WY3LSvJ"
      },
      "outputs": [],
      "source": [
        "saved_model_path = '/content/drive/MyDrive/Company Case Bangkit/TranslationModel/saved_model'\n",
        "\n",
        "translator = Translator(\n",
        "    saved_model_path,\n",
        "    input_tokenizer_dir,\n",
        "    target_tokenizer_dir,\n",
        "    input_maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLjLFWVdFCCQ"
      },
      "outputs": [],
      "source": [
        "text_input = 'i like apple'\n",
        "\n",
        "translate = translator(text_input)\n",
        "print(translate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFpBRO4W31YA"
      },
      "outputs": [],
      "source": [
        "# converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# converter.target_spec.supported_ops = [\n",
        "#   tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "#   tf.lite.OpsSet.SELECT_TF_OPS\n",
        "# ]\n",
        "# converter.experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# tflite_model_file = pathlib.Path('translation.tflite')\n",
        "# tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "# # files.download('translation.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkJJXewBdV2J"
      },
      "outputs": [],
      "source": [
        "# interpreter = tf.lite.Interpreter('translation.tflite')\n",
        "# interpreter.allocate_tensors()\n",
        "\n",
        "# input_details = interpreter.get_input_details()[0]\n",
        "# output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "# input_details, output_details"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequence = input_tokenizer.texts_to_sequences([text_input])\n",
        "# pad_seqs = pad_sequences(\n",
        "#     sequence, maxlen=input_maxlen, padding='post', truncating='post')\n",
        "\n",
        "# input_data = pad_seqs.astype(np.float32)\n",
        "\n",
        "# interpreter.set_tensor(input_details['index'], input_data)\n",
        "# interpreter.invoke()\n",
        "\n",
        "# predictions = interpreter.get_tensor(output_details['index'])\n"
      ],
      "metadata": {
        "id": "glQukvtr02_e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "translator.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "765233bfe060b87a8be23ec8f17030d468ac6435ae34b0ad14370b4cb734ac81"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}